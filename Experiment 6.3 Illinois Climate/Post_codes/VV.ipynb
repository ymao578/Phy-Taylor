{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taylor_nn(prev_layer, weights, biases, act_type, num_of_layers, expansion_order, name ='U'):\n",
    "    \n",
    "    \"\"\"Apply a NN to input from previous later\n",
    "\n",
    "    Arguments:\n",
    "        prev_layer      -- input from previous NN\n",
    "        weights         -- dictionary of weights\n",
    "        biases          -- dictionary of biases (uniform(-1,1) distribution, normal(0,1) distrubution, none--zeros)\n",
    "        act_type        -- dictionary of activation functions (sigmoid, relu, elu, or none): user option \n",
    "        num_of_layers   -- number of weight matrices or layers: user option \n",
    "        expansion_order -- dictionary of Taylor expansion order: user option \n",
    "\n",
    "    Returns:\n",
    "        output of network for input from previous layer\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in np.arange(num_of_layers):\n",
    "        \n",
    "  \n",
    "        #Compressor One###\n",
    "        if i == 2:\n",
    "            prev_layer = tf.nn.relu(prev_layer)\n",
    "        else:\n",
    "            prev_layer = prev_layer\n",
    "#######################################################################################################################       \n",
    "        \n",
    "        \n",
    "        \n",
    "        #Taylor Mapping to procss output from previous layber##############################################################################\n",
    "        \n",
    "        #save raw input###\n",
    "        input_raw = prev_layer\n",
    "        raw_input_shape = input_raw.shape\n",
    "        ###################################################################################################################################\n",
    "        \n",
    "        #The expaned input via Taylor expansion is denoted by input_epd###\n",
    "        input_epd = input_raw\n",
    "        ###################################################################################################################################\n",
    "        \n",
    "        #Anxiliary index###\n",
    "        io = int(raw_input_shape[0])\n",
    "        Id = np.arange(io)\n",
    "        ###################################################################################################################################\n",
    "        \n",
    "        #Nolinear mapping through Taylor expansion###\n",
    "        for _ in range(expansion_order['E%s%d' % (name, i +  1)]):\n",
    "            for j in range(raw_input_shape[0]):\n",
    "                for q in range(raw_input_shape[1]):\n",
    "                    x_temp = np.multiply(input_raw[j,q], input_epd[Id[j]:(Id[raw_input_shape[0]-1]+1),q])\n",
    "                    if q == 0:\n",
    "                        tem_temp = x_temp\n",
    "                    else:\n",
    "                        tem_temp = np.concatenate((tem_temp,x_temp),1)\n",
    "                Id[j] = input_epd.shape[0] \n",
    "                input_epd = np.concatenate((input_epd,tem_temp),0)\n",
    "        ###################################################################################################################################\n",
    "       \n",
    "    \n",
    "        #print(weights['WU1'])\n",
    "        \n",
    "        W = weights['W%s%d' % (name,i + 1)]\n",
    "        Cb  =  biases['b%s%d' % (name,i + 1)] \n",
    "        #print(W.type)\n",
    "        #print(input_epd.shape)\n",
    "        #print(Cb.shape)\n",
    "    \n",
    "        prev_layer = np.matmul(W,input_epd) + Cb \n",
    "        \n",
    "        print(act_type['act%s%d' % (name,i + 1)])\n",
    "        \n",
    "    \n",
    "        \n",
    "        #prev_layer = np.matmul(weights['W%s%d' % (name,i + 1)],input_epd) + biases['b%s%d' % (name,i + 1)] \n",
    "\n",
    "        if act_type['act%s%d' % (name,i + 1)] == 'sigmoid':\n",
    "            prev_layer = tf.sigmoid(prev_layer)\n",
    "        elif act_type['act%s%d' % (name,i + 1)] == 'relu':\n",
    "            prev_layer = tf.nn.relu(prev_layer)\n",
    "        elif act_type ['act%s%d' % (name,i + 1)] == 'elu':\n",
    "            prev_layer = tf.nn.elu(prev_layer)\n",
    "        elif act_type ['act%s%d' % (name,i + 1)] == 'none':\n",
    "            prev_layer = prev_layer\n",
    "        ###################################################################################################################################    \n",
    "        \n",
    "        print('ooooooo')\n",
    "        print(prev_layer)\n",
    "        print('ooooooo')\n",
    "        \n",
    "    return prev_layer\n",
    "###########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Model Parameters#########################################################################################################\n",
    "params = {}\n",
    "input_dim = 12; \n",
    "params['uncheckable_dist_weights'] = ['tn','tn','tn']\n",
    "params['uncheckable_output_size'] = [input_dim,50,5,2]\n",
    "params['uncheckable_epd'] = np.array([0,0,2])\n",
    "params['uncheckable_act'] = ['elu','elu','none']\n",
    "params['uncheckable_dist_biases'] = ['normal','normal','normal']\n",
    "params['uncheckable_num_of_layers'] = len(np.array([0,0,0])) \n",
    "\n",
    "###########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load Weights, bias, activation and expansion order#####################################################################################################\n",
    "WC = dict()\n",
    "WU = dict()\n",
    "bC = dict()\n",
    "bU = dict()\n",
    "act_typeC = dict()\n",
    "act_typeU = dict()\n",
    "expansion_orderC = dict()\n",
    "expansion_orderU = dict()\n",
    "\n",
    "\n",
    "\n",
    "for j in range(params['uncheckable_num_of_layers']):\n",
    "    WU['WU%d' % (j + 1)] = np.matrix(np.loadtxt(\"../b4taylorcomon1d/exp_1_WU%d.csv\" % (j+1), delimiter=',', dtype=np.float64)) \n",
    "    bU['bU%d' % (j + 1)] = np.transpose(np.matrix(np.loadtxt(\"../b4taylorcomon1d/exp_1_bU%d.csv\" % (j+1), delimiter=',', dtype=np.float64)))    \n",
    "    act_typeU['actU%d' % (j + 1)] = params['uncheckable_act'][j]\n",
    "    expansion_orderU['EU%d' % (j + 1)] = params['uncheckable_epd'][j]\n",
    "\n",
    "#for i in range(params['checkable_num_of_layers']):\n",
    "    #if i == 0:\n",
    "        #WC['WC%d' % (i + 1)] = np.transpose(np.matrix(np.loadtxt(\"../taylor_test/exp_1_WC%d.csv\" % (i+1), delimiter=',', dtype=np.float64)))\n",
    "    #else:\n",
    "#    WC['WC%d' % (i + 1)] = np.matrix(np.loadtxt(\"../test0/exp_1_WC%d.csv\" % (i+1), delimiter=',', dtype=np.float64))    \n",
    "#    bC['bC%d' % (i + 1)] = np.transpose(np.matrix(np.loadtxt(\"../test0/exp_1_bC%d.csv\" % (i+1), delimiter=',', dtype=np.float64)))\n",
    "#    act_typeC['actC%d' % (i + 1)] = params['uncheckable_act'][i]\n",
    "#    expansion_orderC['EC%d' % (i + 1)] = params['uncheckable_epd'][i]\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load data############################################################################################################\n",
    "Xtest = np.matrix(np.loadtxt(\"../post/climate_X10.csv\",delimiter=',', dtype=np.float64));\n",
    "Ytest = np.matrix(np.loadtxt(\"../post/ytest.csv\",delimiter=',', dtype=np.float64));\n",
    "#######################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elu\n",
      "ooooooo\n",
      "Tensor(\"Elu_32:0\", shape=(50, 1), dtype=float64)\n",
      "ooooooo\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic Tensor (Elu_32:0) to a numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-7717c3ef139f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mti\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtaylor_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact_typeU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpansion_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpansion_orderU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'U'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-186-e10a35b773a8>\u001b[0m in \u001b[0;36mtaylor_nn\u001b[0;34m(prev_layer, weights, biases, act_type, num_of_layers, expansion_order, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m#print(Cb.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mprev_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_epd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mCb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_type\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'act%s%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     raise NotImplementedError(\"Cannot convert a symbolic Tensor ({}) to a numpy\"\n\u001b[0;32m--> 736\u001b[0;31m                               \" array.\".format(self.name))\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (Elu_32:0) to a numpy array."
     ]
    }
   ],
   "source": [
    "##Create DeepTaylor for a detailed problem################################################################################################\n",
    "\n",
    "#x = np.transpose(Xtest[863,:])\n",
    "\n",
    "zz = 2\n",
    "kf = 0\n",
    "\n",
    "Y = np.empty(zz, dtype=object)\n",
    "\n",
    "\n",
    "ti = 1\n",
    "\n",
    "for i in range(zz):\n",
    "    \n",
    "    x = np.transpose(Xtest[ti+i,:])\n",
    "    y = taylor_nn(prev_layer=x, weights=WU, biases=bU, act_type=act_typeU, num_of_layers=3, expansion_order=expansion_orderU, name='U')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "########################################################################################################################################### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Model Parameters#########################################################################################################\n",
    "params = {}\n",
    "input_dim = 3; \n",
    "params['uncheckable_dist_weights'] = ['tn']\n",
    "params['uncheckable_output_size'] = [input_dim,3]\n",
    "params['uncheckable_epd'] = np.array([4])\n",
    "params['uncheckable_act'] = ['none']\n",
    "params['uncheckable_dist_biases'] = ['uniform']\n",
    "params['uncheckable_num_of_layers'] = len(np.array([0])) \n",
    "\n",
    "params['checkable_dist_weights'] = ['tn']\n",
    "params['checkable_output_size'] = [3,3]\n",
    "params['checkable_epd'] = np.array([4])\n",
    "params['checkable_act'] = ['none']\n",
    "params['checkable_dist_biases'] = ['none']\n",
    "params['checkable_num_of_layers'] = len(np.array([0])) \n",
    "###########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "../test4/exp_1_WU1.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-468abbe6138e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uncheckable_num_of_layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mWU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WU%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../test4/exp_1_WU%d.csv\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mbU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bU%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../test4/exp_1_bU%d.csv\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mact_typeU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actU%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uncheckable_act'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    621\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ../test4/exp_1_WU1.csv not found."
     ]
    }
   ],
   "source": [
    "##Load Weights, bias, activation and expansion order#####################################################################################################\n",
    "WC = dict()\n",
    "WU = dict()\n",
    "bC = dict()\n",
    "bU = dict()\n",
    "act_typeC = dict()\n",
    "act_typeU = dict()\n",
    "expansion_orderC = dict()\n",
    "expansion_orderU = dict()\n",
    "\n",
    "\n",
    "\n",
    "for j in range(params['uncheckable_num_of_layers']):\n",
    "\n",
    "    WU['WU%d' % (j + 1)] = np.matrix(np.loadtxt(\"../test4/exp_1_WU%d.csv\" % (j+1), delimiter=',', dtype=np.float64)) \n",
    "    bU['bU%d' % (j + 1)] = np.transpose(np.matrix(np.loadtxt(\"../test4/exp_1_bU%d.csv\" % (j+1), delimiter=',', dtype=np.float64)))\n",
    "    act_typeU['actU%d' % (j + 1)] = params['uncheckable_act'][j]\n",
    "    expansion_orderU['EU%d' % (j + 1)] = params['uncheckable_epd'][j]\n",
    "\n",
    "for i in range(params['checkable_num_of_layers']):\n",
    "    #if i == 0:\n",
    "        #WC['WC%d' % (i + 1)] = np.transpose(np.matrix(np.loadtxt(\"../taylor_test/exp_1_WC%d.csv\" % (i+1), delimiter=',', dtype=np.float64)))\n",
    "    #else:\n",
    "    WC['WC%d' % (i + 1)] = np.matrix(np.loadtxt(\"../test4/exp_1_WC%d.csv\" % (i+1), delimiter=',', dtype=np.float64))    \n",
    "    bC['bC%d' % (i + 1)] = np.transpose(np.matrix(np.loadtxt(\"../test4/exp_1_bC%d.csv\" % (i+1), delimiter=',', dtype=np.float64)))\n",
    "    act_typeC['actC%d' % (i + 1)] = params['uncheckable_act'][i]\n",
    "    expansion_orderC['EC%d' % (i + 1)] = params['uncheckable_epd'][i]\n",
    "############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create DeepTaylor for a detailed problem################################################################################################\n",
    "x = np.transpose(np.matrix([1.45932112626739,-0.40749775220646,-0.00596193701846]))\n",
    "\n",
    "Y = np.empty(15, dtype=object)\n",
    "Z = np.empty(15, dtype=object)\n",
    "\n",
    "for i in range(15):\n",
    "\n",
    "    y = taylor_nn(prev_layer=x, weights=WU, biases=bU, act_type=act_typeU, \n",
    "                  num_of_layers=params['uncheckable_num_of_layers'], expansion_order=expansion_orderU,name='U')\n",
    "    z = taylor_nn(prev_layer=y, weights=WC, biases=bC, act_type=act_typeC, \n",
    "                  num_of_layers=params['checkable_num_of_layers'], expansion_order=expansion_orderC,name='C')\n",
    "    \n",
    "    Y[i] = y[0]\n",
    "    Z[i] = z[0]\n",
    "    \n",
    "    x = y\n",
    "\n",
    "\n",
    "\n",
    "print(Y)\n",
    "print(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
